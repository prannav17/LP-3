# -*- coding: utf-8 -*-
"""Pract - 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P_TeRjDVPkt1cWhwTNt73Fa3alhWSmtd
"""

"""Gradient Descent Implementation"""

cur_x = 2  # The algorithm starts at x=2
rate = 0.01  # Learning rate
precision = 0.000001  # This tells us when to stop the algorithm
previous_step_size = 1
max_iters = 10000  # maximum number of iterations
iters = 0  # iteration counter

# Store values for plotting
x_values = []
y_values = []

# Gradient of our function
df = lambda x: 2 * (x + 3)

while previous_step_size > precision and iters < max_iters:
    prev_x = cur_x  # Store current x value in prev_x
    cur_x = cur_x - rate * df(prev_x)  # Gradient descent
    previous_step_size = abs(cur_x - prev_x)  # Change in x
    iters += 1  # iteration count

    # Store values for plotting
    x_values.append(cur_x)
    y_values.append((cur_x + 3) ** 2)  # f(x) = (x + 3)^2

    print("Iteration", iters, "\nX value is", cur_x)  # Print iterations

print("The local minimum occurs at", cur_x)

import numpy as np
import matplotlib.pyplot as plt

# Define the function for plotting
f = lambda x: (x + 3) ** 2

# Create an array of x values for the function plot
x_range = np.linspace(-10, 5, 100)

plt.figure(figsize=(10, 6))
plt.plot(x_range, f(x_range), label='f(x) = (x + 3)^2', color='blue')  # Function plot
plt.scatter(x_values, y_values, color='red', label='Gradient Descent Steps')  # Points from gradient descent
plt.title('Gradient Descent Optimization')
plt.xlabel('X values')
plt.ylabel('f(X)')
plt.axhline(0, color='black', linewidth=0.5, ls='--')
plt.axvline(0, color='black', linewidth=0.5, ls='--')
plt.grid()
plt.legend()
plt.show()

